{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My Drive/Colab Notebooks/hw3"
      ],
      "metadata": {
        "id": "QiYEF-Fy92ky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a48ba8b9-45b3-49d2-b56d-e29642f5ac47"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Colab Notebooks/hw3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 1:  Sequence Tagging with RNNs**\n",
        "\n",
        "In this task, you will implement LSTM and Bi-LSTM architectures with PyTorch to perform part-of-speech tagging (a sequence tagging task).\n",
        "\n",
        "### **Data**\n",
        "We use a subset of the data from the CoNLL-2003 shared task on Named Entity Recognition (provided in the zip). It is pre-partioned into a training, development and test set.\n",
        "\n",
        "The dataset consists of pre-tokenized sentences where every token is annotated with a part-of-speech tag, a syntactic chunk tag and a named entity tag. In this home exercise, we only use the IOB named entity recognition tag."
      ],
      "metadata": {
        "id": "7g3LeIyQ9-kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "zEJqyHIV3bC2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task 1.1: Pretrained Embeddings (5p)**\n",
        "\n",
        "Download the pretrained, uncased GloVe embeddings with 6B tokens [glove.6B.zip](https://nlp.stanford.edu/projects/glove/) from Stanford.\n",
        "\n",
        "For performance reasons, we will only use the 50-dimensional embeddings **glove.6B.50d.txt**.\n",
        "\n",
        "Implement a function to read the embedding and another function to read the dataset."
      ],
      "metadata": {
        "id": "Nb1gCI-f-mJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://medium.com/analytics-vidhya/ner-tensorflow-2-2-0-9f10dcf5a0a\n",
        "\"\"\"\n",
        "  Store each sentence seperately. Each token of the sentences is stored with its corresponding IOB named entity recognition tag\n",
        "\"\"\"\n",
        "\n",
        "def split_text_label(filename):\n",
        "  f = open(filename)\n",
        "  split_labeled_text = []\n",
        "  sentence = []\n",
        "  for line in f:\n",
        "    if len(line)==0 or line.startswith('-DOCSTART') or line[0]==\"\\n\":\n",
        "       if len(sentence) > 0:\n",
        "         split_labeled_text.append(sentence)\n",
        "         sentence = []\n",
        "       continue\n",
        "    splits = line.split(' ')\n",
        "    sentence.append([splits[0],splits[-1].rstrip(\"\\n\")])\n",
        "  if len(sentence) > 0:\n",
        "    split_labeled_text.append(sentence)\n",
        "    sentence = []\n",
        "  return split_labeled_text"
      ],
      "metadata": {
        "id": "z4L2qEqct9KE"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(path):\n",
        "    data = defaultdict(list)\n",
        "    label = defaultdict(list)\n",
        "    # TODO: YOUR CODE HERE\n",
        "\n",
        "    text_label = split_text_label(filename=path)\n",
        "    for idx, sent in enumerate(text_label):\n",
        "      words = []\n",
        "      iob_labels = []\n",
        "      for word_iob in sent:\n",
        "        words.append(word_iob[0]) #add word\n",
        "        iob_labels.append(word_iob[1]) #add iob tag\n",
        "\n",
        "      data[idx] = words\n",
        "      label[idx] = iob_labels\n",
        "\n",
        "    return data, label\n",
        "\n",
        "def get_pretrained_embeddings(embedding_path):\n",
        "    embeddings = defaultdict(list)\n",
        "    # TODO: YOUR CODE HERE\n",
        "    with open(\"glove.6B.50d.txt\", \"r\") as file:\n",
        "      for line in file:\n",
        "        line = line.strip()\n",
        "        word = line.split(\" \")[0]\n",
        "        vector = np.array([float(n) for n in line.split(\" \")[1:]])\n",
        "\n",
        "        embeddings[word] = vector\n",
        "\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "gmpt_ya90sJw"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed = get_pretrained_embeddings(embedding_path=\"glove.6B.50d.txt\")"
      ],
      "metadata": {
        "id": "0xB0-Gc9mQXU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed[\"get\"].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KItfD8d-mZhn",
        "outputId": "6c1e7572-7c6e-4a70-c64e-8ebd1b583a17"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50,)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out= split_text_label(\"data/ner_eng_bio.train\")"
      ],
      "metadata": {
        "id": "L5TTPibcuKNS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJIb2BsHuPvm",
        "outputId": "35e0e26d-1528-4fe9-d75e-8ce672f3b5a9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['BRUSSELS', 'B-LOC'], ['1996-08-22', 'O']]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data, label = read_data(\"data/ner_eng_bio.train\")"
      ],
      "metadata": {
        "id": "UeK8Sad-xvW2"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7k8llI_Dx0iX",
        "outputId": "2b511d09-b77c-4266-a1fa-1715fb38fe9b"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsvfI0oryAje",
        "outputId": "9ab991e3-6344-4179-c48c-1404cadcdeb4"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counter = 0\n",
        "with open(\"data/ner_eng_bio.train\", \"r\") as file:\n",
        "    for line in file:\n",
        "      print(line.strip().split(\" \"))\n",
        "      if counter == 10:\n",
        "        break\n",
        "      counter += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjz1RhREhbLz",
        "outputId": "0047c771-57c9-43f4-ef0f-84806cc41bb9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['EU', 'NNP', 'B-NP', 'B-ORG']\n",
            "['rejects', 'VBZ', 'B-VP', 'O']\n",
            "['German', 'JJ', 'B-NP', 'B-MISC']\n",
            "['call', 'NN', 'I-NP', 'O']\n",
            "['to', 'TO', 'B-VP', 'O']\n",
            "['boycott', 'VB', 'I-VP', 'O']\n",
            "['British', 'JJ', 'B-NP', 'B-MISC']\n",
            "['lamb', 'NN', 'I-NP', 'O']\n",
            "['.', '.', 'O', 'O']\n",
            "['']\n",
            "['Peter', 'NNP', 'B-NP', 'B-PER']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"dataset = defaultdict(list)\n",
        "labels = defaultdict(list)\n",
        "with open(\"data/ner_eng_bio.train\", \"r\") as file:\n",
        "    sentence = []\n",
        "    sentence_labels = []\n",
        "    for line in file:\n",
        "        line = line.strip()\n",
        "        if line == \"\":\n",
        "            if sentence:\n",
        "                for word, label in zip(sentence, sentence_labels):\n",
        "                    dataset[\"words\"].append(word)\n",
        "                    labels[\"tags\"].append(label)\n",
        "            sentence = []\n",
        "            sentence_labels = []\n",
        "        else:\n",
        "            word, _, _, label = line.split()\n",
        "            sentence.append(word)\n",
        "            sentence_labels.append(label)\"\"\""
      ],
      "metadata": {
        "id": "E0CcQMKRjvkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task 1.2: LSTM and Bi-LSTM Model (10p)**\n",
        "\n",
        "We will use PyTorch to build our LSTM. Complete the `__init__()` and the `forward()` function of the CustomLSTM class. The model will have the following components:\n",
        "- A single [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) layer which takes the embeddings as input and has 100-dimensional hidden layer. The LSTM is **not** bidirectional.\n",
        "- A dropout layer with probability 0.1\n",
        "- A linear layer with input size of 100 (the hidden layer size of the LSTM layer) and output size of the number of labels\n",
        "- A [Sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html) activation function"
      ],
      "metadata": {
        "id": "fCdGcnwv_f7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules import dropout\n",
        "class CustomLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_labels):\n",
        "        super(CustomLSTM, self).__init__()\n",
        "        # TODO: YOUR CODE HERE\n",
        "        \"\"\"self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.num_labels = num_labels\"\"\"\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=0.1, bidirectional=False)\n",
        "        self.linear = nn.Linear(input_size=100, out_features=num_labels)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: YOUR CODE HERE\n",
        "        #LSTM’s output corresponding to all timesteps\n",
        "        output, (h_n, c_n) = self.lstm(x)\n",
        "        last_hidden_state = output[-1] # can also be h_n   see https://towardsdatascience.com/implementation-differences-in-lstm-layers-tensorflow-vs-pytorch-77a31d742f74#:~:text=The%20output%20of%20the%20Pytorch,another%20tuple%20with%20two%20elements.\n",
        "        linear_output = self.linear(last_hidden_state)\n",
        "        output = self.sigmoid(linear_output)\n",
        "        \n",
        "        return output"
      ],
      "metadata": {
        "id": "Z9QoXzQSoiBS"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task 1.3: Training Model (10p)**\n",
        "\n",
        "Complete the function `train` to train your model. The model will train with batch size of 1 (each sentence split by \"\\n\" is a sample) for 10 epochs. You will train the model with the train dataset and use dev dataset to check the model's performance after each epoch. Calculate the macro f1 score of the model on the dev set. Return the losses and f1 scores for plotting.\n",
        "\n",
        "**Hint**: you can check out this [link](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html#the-training-loop) to get to know more about how to train model with pytorch. For the f1 score you can use `sklearn.metrics.f1_score`, remember to set the `average` parameter to \"macro\".\n"
      ],
      "metadata": {
        "id": "egNWahhpoiu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def label_encode(labels):\n",
        "    labels_list = {'B-LOC': 0, 'B-MISC': 1, 'B-ORG': 2, 'B-PER': 3, 'I-LOC': 4, 'I-MISC': 5, 'I-ORG': 6, 'I-PER': 7,\n",
        "                   'O': 8}\n",
        "    labels_encode = [labels_list[label] for label in labels]\n",
        "    return labels_encode"
      ],
      "metadata": {
        "id": "fQLhObb_HJn0"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change this to your path to the dataset\n",
        "train = \"data/ner_eng_bio.train\" \n",
        "test = \"data/ner_eng_bio.test\"\n",
        "dev = \"data/ner_eng_bio.dev\"\n",
        "\n",
        "embeddings = get_pretrained_embeddings(\"embeddings/glove.6B.50d.txt\")\n",
        "\n",
        "train_data, train_label = read_data(train)\n",
        "test_data, test_label = read_data(test)\n",
        "dev_data, dev_label = read_data(dev)\n",
        "\n",
        "# Change the Hyperparameters here\n",
        "input_size = 0\n",
        "hidden_size = 0\n",
        "num_layers = 0\n",
        "num_labels = 0\n",
        "epochs = 0"
      ],
      "metadata": {
        "id": "Jg4WnZx2dej3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomLSTM(input_size, hidden_size, num_layers, num_labels)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters()) \n",
        "\n",
        "def train(model, loss_fn, optimizer, train_data, dev_data, epochs):\n",
        "    logs_loss = []\n",
        "    logs_f1_score = []\n",
        "\n",
        "    # Training\n",
        "    for epoch in range(epochs):\n",
        "        # Train with train set\n",
        "        model.train()\n",
        "        # TODO: YOUR CODE HERE\n",
        "\n",
        "        # Evaluate with dev set\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            # TODO: YOUR CODE HERE\n",
        "        \n",
        "    return logs_loss, logs_f1_score\n",
        "\n",
        "logs_loss, logs_f1_score = train(model, loss_fn, optimizer, train_data, dev_data, epochs)"
      ],
      "metadata": {
        "id": "WJreNFlF18BK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task 1.4: Visualizing (5p)**\n",
        "\n",
        "Check the performance of the model on the test set and plot the training loss using `matplotlib.pyplot.plot`."
      ],
      "metadata": {
        "id": "xz6uyUXFWnAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate with test set\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    targets = []\n",
        "    # TODO: YOUR CODE HERE\n",
        "\n",
        "# Plot with matplotlib\n"
      ],
      "metadata": {
        "id": "enRLWn887ytw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}