{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My Drive/Colab Notebooks/hw4"
      ],
      "metadata": {
        "id": "W_35WKkRgRDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "3vJcrRscg5XA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, AutoTokenizer, GPT2LMHeadModel\n",
        "from datasets import load_dataset\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "FWXQ59Qrg89R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task 1: Byte pair encoding**"
      ],
      "metadata": {
        "id": "zHB7g4M9BDm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal is to calculate byte pair encodings for a given dataset. For a recap of the concept we refer to lecture 9, slides 22.ff and internet search."
      ],
      "metadata": {
        "id": "HfcrKwekNUdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.1\n",
        "First we want to make sure we create a suffieciently large corpus. As dataset you import the dataset \"wikitext-2-raw-v1\" from huggingface. Make sure to use the 500 first entries of the training dataset and subsequently filter out empty  entries. Also print the first entry of the corpus."
      ],
      "metadata": {
        "id": "HcSdEAeKtQJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load WikiText-2 dataset \"wikitext-2-raw-v1\"\n",
        "corpus = []\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "ekOh5k7rtA3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the corpus should have a format like\n",
        "corpus = [\n",
        "    \"This is the first sentence\",\n",
        "    \"This is the second sentence\",\n",
        "    ..\n",
        "]. For debugging during coding you can also consider working with a smaller corpus, e.g. only 5 sentences"
      ],
      "metadata": {
        "id": "vUdYyNyavtOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.2\n",
        "Here, you loop through the corpus and count the word frequencies"
      ],
      "metadata": {
        "id": "D6BN0JoquLz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a tokenizer from the transformers library\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Initialize a defaultdict to store word frequencies\n",
        "word_freqs = defaultdict(int)\n",
        "# this dictionary gets entries for word_freqs[word]\n",
        "\n",
        "alphabet = []\n",
        "vocab = [\"\"]\n",
        "splits = {}\n",
        "vocab_size = 200\n",
        "merges = {}\n",
        "\n",
        "# We loop through to corups to calculate word frequencies\n",
        "for text in corpus:\n",
        "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    new_words = [word for word, offset in words_with_offsets]\n",
        "    # YOUR CODE HERE to count word frequencies\n",
        "\n"
      ],
      "metadata": {
        "id": "YywJmC5uyWsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following block creates the alphabet and initial vocabulary"
      ],
      "metadata": {
        "id": "Z7UQXeML0cVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word in word_freqs.keys():\n",
        "    for letter in word:\n",
        "        if letter not in alphabet:\n",
        "            alphabet.append(letter)\n",
        "alphabet.sort()\n",
        "\n",
        "print(\"alphabet\",alphabet)\n",
        "vocab = [\"<|endoftext|>\"] + alphabet.copy()\n",
        "splits = {word: [c for c in word] for word in word_freqs.keys()}"
      ],
      "metadata": {
        "id": "GSqR2eMYzmo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.3\n",
        "Define a function to compute the frequency of each pair of characters"
      ],
      "metadata": {
        "id": "xlxGKcZTzLZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to compute the frequency of each pair of characters\n",
        "def compute_pair_freqs(splits):\n",
        "    pair_freqs = defaultdict(int)\n",
        "    # YOUR CODE HERE to compute the frequency of each pair of characters\n",
        "\n",
        "    return pair_freqs\n",
        "\n",
        "# Initialize pair_freqs by calling the compute_pair_freqs function\n",
        "pair_freqs = compute_pair_freqs(splits)"
      ],
      "metadata": {
        "id": "7SBWAIBl0_ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we find the pair of characters that appears most frequently together"
      ],
      "metadata": {
        "id": "Yjg9JO4R1TQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, key in enumerate(pair_freqs.keys()):\n",
        "    print(f\"{key}: {pair_freqs[key]}\")\n",
        "    if i >= 5:\n",
        "        break\n",
        "\n",
        "best_pair = \"\"\n",
        "max_freq = None\n",
        "\n",
        "for pair, freq in pair_freqs.items():\n",
        "    if max_freq is None or max_freq < freq:\n",
        "        best_pair = pair\n",
        "        max_freq = freq\n",
        "\n",
        "print(best_pair, max_freq)"
      ],
      "metadata": {
        "id": "wrEE3mQn1Ley"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.4\n",
        "Define a function to merge the most frequent pair of characters"
      ],
      "metadata": {
        "id": "t66VBj9E1je1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define a function to merge the most frequent pair of characters\n",
        "def merge_pair(a, b, splits):\n",
        "    for word in word_freqs:\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            continue\n",
        "\n",
        "        # YOUR CODE HERE to merge the most frequent pair of characters in each word\n",
        "\n",
        "    return splits\n"
      ],
      "metadata": {
        "id": "H80PfjRHBJxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.5\n",
        "\n",
        "Iterate this process until you reach a predefined vocabulary size"
      ],
      "metadata": {
        "id": "9bn6OgVT1wD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE to iterate this process until you reach a predefined vocabulary size\n",
        "\n"
      ],
      "metadata": {
        "id": "X2XR_lXh1vou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We provide the tokenize function to you"
      ],
      "metadata": {
        "id": "j8m-wRcB2EBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
        "    splits = [[l for l in word] for word in pre_tokenized_text]\n",
        "    for pair, merge in merges.items():\n",
        "        for idx, split in enumerate(splits):\n",
        "            i = 0\n",
        "            while i < len(split) - 1:\n",
        "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
        "                    split = split[:i] + [merge] + split[i + 2 :]\n",
        "                else:\n",
        "                    i += 1\n",
        "            splits[idx] = split\n",
        "\n",
        "    return sum(splits, [])\n",
        "tokenize(\"Darmstadt holds the official title City of Science (German: Wissenschaftsstadt) as it is a major centre of scientific institutions, universities, and high-technology companies.\")"
      ],
      "metadata": {
        "id": "cblln3YG2Dbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.6\n",
        "Now, repeat the byte pair encoding for vocab sizes of 1000 and 5000. What do you notice?"
      ],
      "metadata": {
        "id": "7hhrzhK92Snp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this chunk for the answer"
      ],
      "metadata": {
        "id": "meRyvt-02ydt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this chunk for the answer"
      ],
      "metadata": {
        "id": "kal8RQFO2261"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 2: Getting to Know Foundation Models**"
      ],
      "metadata": {
        "id": "5ghNlhsff965"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this task, we want to load two different versions of GPT-2 and compare their performance."
      ],
      "metadata": {
        "id": "aV17kklOgivY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.1\n",
        "\n",
        "After we imported transformers from Huggingface, your first task is to initialize the GPT2LMHead Model. This model should be a gpt2 model from huggingface under the variable model. Also define the GPT2 tokenizer.\n",
        "\n",
        "While we have seen the transformers in the lectures, you should print a single GPT2Block to the console for the 10th layer. Then, count the parameters of the model."
      ],
      "metadata": {
        "id": "USWKMnuJhF9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a GPT2Block in the 10th layer\n",
        "\n",
        "def count_parameters(model):\n",
        "    #YOUR CODE HERE\n",
        "\n",
        "# GPT2_small parameter trainable parameters:\n",
        "print(f\"The model has {count_parameters(model_small):,} trainable parameters\")"
      ],
      "metadata": {
        "id": "oFrpwjZ7iUv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.2\n",
        "Because we want to compare two versions of GPT, you now also load the gpt2-large from huggingface as model. As evaluation dataset you import the dataset \"wikitext-2-raw-v1\" from huggingface. (like in Task1) Make sure to use the 200 first entries of the test dataset and print an entry. Also print the parameters of the gpt2-large model"
      ],
      "metadata": {
        "id": "K5aP7qz1jTB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the gpt2-large model from huggingface\n",
        "\n",
        "# Load the dataset\n",
        "\n",
        "# Print any single entry\n",
        "\n",
        "# Print Trainable parameters of huggingface gpt2-large"
      ],
      "metadata": {
        "id": "o65wiL-ljSXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.3\n",
        "As a next step, we define an evaluation metric to calculate predictions. Remember the perplexity funciton from the lecture. The function should take a model, tokenizer and a text and return the perplexity score for the given text.\n",
        "\n",
        "Hint: you should encode the text, generate outputs for inputs and corresponding labels and then calculate perplexity."
      ],
      "metadata": {
        "id": "PE8eAYwllUXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_perplexity(model, tokenizer, text):\n",
        "    # YOUR CODE HERE"
      ],
      "metadata": {
        "id": "GoW8ALAclTNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perplexities_small = []\n",
        "perplexities_large = []\n",
        "# Loop over the dataset, append scores to the lists\n",
        "\n",
        "# Calculate and print average perplexities\n",
        "print(\"Average perplexity (GPT2 ): \", np.nanmean(perplexities_small))\n",
        "print(\"Average perplexity (GPT2 large): \", np.nanmean(perplexities_large))"
      ],
      "metadata": {
        "id": "qQTvRJXdnKh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do you notice? Please explain."
      ],
      "metadata": {
        "id": "rZo_eKSyo3Xv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.4 Masking attention\n",
        "What is an attention mask? When and why is it usually used?"
      ],
      "metadata": {
        "id": "rVsco9sDpFuq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your answer here:"
      ],
      "metadata": {
        "id": "Ab6inZye-TbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task is now to run a prompt in regular inference, but with an attention mask. You can reuse the gpt2_small model"
      ],
      "metadata": {
        "id": "PPaOYCMj_EB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_prompt = \"The sun was setting behind\"\n",
        "\n",
        "# Step 1: Implement attention masking\n",
        "def apply_attention_mask(input_ids, mask_start_idx, mask_end_idx):\n",
        "    # -----------------------\n",
        "    #YOUR CODE HERE\n",
        "    # -----------------------\n",
        "\n",
        "    #return attention_mask\n",
        "\n",
        "# Step 2: Apply attention masks\n",
        "mask_start = 3  # Start index of the region to mask\n",
        "mask_end = 5  # End index of the region to mask\n",
        "input_ids = tokenizer_small.encode(text_prompt, return_tensors='pt')\n",
        "attention_mask = apply_attention_mask(input_ids, mask_start, mask_end)\n",
        "\n",
        "# Step 3: Generate output using modified attention mechanism\n",
        "with torch.no_grad():\n",
        "    outputs = model_small.generate(input_ids, attention_mask=attention_mask)\n",
        "    generated_text = tokenizer_small.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Step 4: Compare and analyze the generated outputs\n",
        "print(\"Text Prompt:\", text_prompt)\n",
        "print(\"Generated Text:\", generated_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "_sKRNynD_DTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Run the same prompt without attention mask\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "H5QQUlgY3XOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Compare the two outputs and describe it in one to two sentences"
      ],
      "metadata": {
        "id": "-cBpnyCRAD5v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your answer here:"
      ],
      "metadata": {
        "id": "DWIZC2WAAEXT"
      }
    }
  ]
}